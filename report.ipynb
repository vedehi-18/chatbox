{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2f6089",
   "metadata": {},
   "source": [
    "# Report: Core Algorithm Behind Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc676e2",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) like GPT, BERT, and LLaMA are among the most impactful developments in artificial intelligence. These models are built on a foundational algorithm known as the Transformer. Introduced in 2017, the Transformer architecture has reshaped how machines understand and generate human language by leveraging attention mechanisms to process textual data more effectively than previous models such as RNNs and LSTMs.\n",
    "\n",
    "This report explores the core algorithm used in LLMs from both theoretical and practical coding perspectives, providing an in-depth understanding of each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5cb31",
   "metadata": {},
   "source": [
    "# Overview of the Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb5d77",
   "metadata": {},
   "source": [
    "The Transformer architecture is designed to handle sequential data using attention mechanisms rather than recurrence. It consists primarily of:\n",
    "\n",
    "Self-Attention Layers\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "Feedforward Neural Networks\n",
    "\n",
    "Positional Encoding\n",
    "\n",
    "Layer Normalization and Residual Connections\n",
    "\n",
    "In LLMs like GPT, a decoder-only Transformer stack is used. This means only the self-attention mechanism is employed without an encoder-decoder structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0530776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (C ** 0.5)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = attn_weights @ v\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))  # Residual + attention\n",
    "        x = x + self.ff(self.ln2(x))    # Residual + MLP\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7b883",
   "metadata": {},
   "source": [
    "# Transformer Block Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34170917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8f2a3",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3e402",
   "metadata": {},
   "source": [
    "Transformers lack a sense of order in the input sequence. Positional encoding is used to inject information about the relative or absolute position of the tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1a5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545c368",
   "metadata": {},
   "source": [
    "# Training Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d48144",
   "metadata": {},
   "source": [
    "Objective Function\n",
    "\n",
    "Most LLMs use next-token prediction, a form of causal language modeling:\n",
    "\n",
    "Input: Sequence of tokens\n",
    "\n",
    "Output: Probability distribution of the next token\n",
    "\n",
    "Loss Function\n",
    "\n",
    "Cross-entropy loss is used to compare predicted probabilities with actual tokens.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "Adam or AdamW is commonly used\n",
    "\n",
    "Learning rate schedules and warm-up steps improve convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c422d5",
   "metadata": {},
   "source": [
    "# Applications of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb936230",
   "metadata": {},
   "source": [
    "LLMs have been applied in diverse areas including:\n",
    "\n",
    "Chatbots and AI Assistants\n",
    "\n",
    "Code generation (e.g., GitHub Copilot)\n",
    "\n",
    "Text summarization\n",
    "\n",
    "Translation\n",
    "\n",
    "Semantic search\n",
    "\n",
    "They have become integral in both research and production AI systems across industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4636f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
